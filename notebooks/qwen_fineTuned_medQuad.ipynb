{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60772c3-a17b-4c8b-8005-4513f92d28aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Tokenizes prior to training\n",
    "# - Adds data collator to produce labels\n",
    "# - Uses BitsAndBytesConfig + device_map auto with safe fallback\n",
    "# - Memory-conscious defaults (change constants below as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75a13a-533f-4e9d-a7cf-c200ecd5e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install deps (quiet)\n",
    "! pip install -q transformers accelerate datasets bitsandbytes peft sentencepiece safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2a1a2-c67e-4c23-bf31-3ca394637cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Optional: reduce fragmentation before allocations\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "# speed/memory tradeoffs\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87383a-368b-4ed8-a60e-5e7d8bc655b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# User tuneable constants\n",
    "# -------------------------\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  #s maller Qwen for 14GB GPU\n",
    "MAX_LENGTH = 1024        # try 512 if you still hit OOM\n",
    "LORA_R = 8               # lower to 4 to save more memory\n",
    "TRAIN_STEPS = 600\n",
    "PER_DEVICE_BATCH = 1\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "OUTPUT_DIR = \"./qwen-medquad-output\"\n",
    "CSV_PATH = \"/content/medquad.csv\"   # path to your CSV\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e84b4-2829-4e29-b9ed-60b96ed57a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 0) Basic checks\n",
    "# -------------------------\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb7af59-951d-47f6-8026-61345a68fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1) Load dataset\n",
    "# -------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": CSV_PATH})\n",
    "dataset = dataset[\"train\"]\n",
    "print(\"Rows:\", len(dataset))\n",
    "print(\"Example:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e2d5b-ca74-4b53-9dfb-ec2369f84f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2) Format chat entries\n",
    "# -------------------------\n",
    "def format_chat(example):\n",
    "    q = example.get(\"question\", \"\") or \"\"\n",
    "    a = example.get(\"answer\", \"\") or \"\"\n",
    "    example[\"text\"] = (\n",
    "        \"<|im_start|>user\\n\"\n",
    "        \"You are a safe and helpful medical assistant.\\n\"\n",
    "        f\"Question: {q}\\n\"\n",
    "        \"<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "        f\"{a}\\n\"\n",
    "        \"<|im_end|>\"\n",
    "    )\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(format_chat)\n",
    "# keep only text column\n",
    "dataset = dataset.remove_columns([c for c in dataset.column_names if c != \"text\"])\n",
    "dataset = dataset.train_test_split(test_size=0.02)\n",
    "print(\"Train / Eval sizes:\", len(dataset[\"train\"]), len(dataset[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceaf680-4aa3-43e5-893c-d48a93173287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3) Load tokenizer\n",
    "# -------------------------\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "# ensure pad token exists and align config\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e68e07-1ccd-4150-94e4-ce164fdeb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 4) Tokenize dataset\n",
    "# -------------------------\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Optionally do a tiny smoke test subset\n",
    "# tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].select(range(200))\n",
    "# tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94953d4a-2685-45c8-a0f5-dac6e1748873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 5) Prepare BitsAndBytesConfig + load model safely\n",
    "# -------------------------\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # compute dtype: try \"bfloat16\" on supported HW, else \"float16\"\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    # this flag helps when device_map auto wants to offload to CPU/disk\n",
    "    # it allows some FP32 cpu offload paths; it's used for int8 but helpful fallback\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "# Try auto device_map (fastest) with quantization_config; if HF complains about module placement, fallback\n",
    "model = None\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"Loaded model with device_map='auto'\")\n",
    "except Exception as e:\n",
    "    print(\"Auto load failed:\", e)\n",
    "    print(\"Falling back to loading all on CUDA:0 (may OOM).\")\n",
    "    # Fallback: try to load everything on GPU 0\n",
    "    if torch.cuda.is_available():\n",
    "        device_map = {\"\": 0}\n",
    "    else:\n",
    "        device_map = {\"\": \"cpu\"}\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=device_map,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"Loaded with explicit device_map:\", device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b403e-741a-4e01-aabb-204c3bbdcc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 6) Apply LoRA (PEFT)\n",
    "# -------------------------\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # good defaults for Qwen-like\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable gradient checkpointing (saves GPU mem)\n",
    "try:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "except Exception:\n",
    "    # some wrappers may not support enable_input_require_grads\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840120d5-4204-4dd3-bdc2-f1d78210d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 7) Data collator -> creates labels for causal LM\n",
    "# -------------------------\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1bec5c-119a-4702-8f15-dcc7e2c5bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 8) Trainer + TrainingArguments\n",
    "# -------------------------\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    max_steps=TRAIN_STEPS,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    report_to=\"none\",\n",
    "    # remove_unused_columns default True is fine since we tokenized\n",
    ")\n",
    "\n",
    "# free some GPU memory before Trainer allocs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],   # set to None to skip eval if OOM\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca25a2-6ad2-4bf2-8383-7adb12fd9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 9) Train\n",
    "# -------------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e9409-a3a2-43cf-a8d3-d99901af974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 10) Merge adapters and save\n",
    "# -------------------------\n",
    "# PEFT: merge adapters (makes a standalone model) and save the merged model & tokenizer\n",
    "try:\n",
    "    merged = model.merge_and_unload()\n",
    "    merged.save_pretrained(os.path.join(OUTPUT_DIR, \"merged\"))\n",
    "    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"merged\"))\n",
    "    print(\"Saved merged model at\", os.path.join(OUTPUT_DIR, \"merged\"))\n",
    "except Exception as e:\n",
    "    print(\"Warning: could not merge adapters (maybe running in-place). Saving PEFT weights only.\")\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ccbd4-fab1-4aaa-8fc9-fecd4c721e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 11) Quick inference sanity-check\n",
    "# -------------------------\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "mp = os.path.join(OUTPUT_DIR, \"merged\") if os.path.isdir(os.path.join(OUTPUT_DIR, \"merged\")) else OUTPUT_DIR\n",
    "tok = AutoTokenizer.from_pretrained(mp, trust_remote_code=True)\n",
    "m = AutoModelForCausalLM.from_pretrained(mp, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "def ask_medical(question: str):\n",
    "    prompt = \"<|im_start|>user\\n\" + question + \"\\n<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(m.device)\n",
    "    out = m.generate(**inputs, max_new_tokens=256, temperature=0.2, do_sample=False)\n",
    "    txt = tok.decode(out[0], skip_special_tokens=True)\n",
    "    return txt.split(\"<|im_start|>assistant\")[-1].strip()\n",
    "\n",
    "print(ask_medical(\"What are the symptoms of iron deficiency anemia?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda]",
   "language": "python",
   "name": "conda-env-.conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
